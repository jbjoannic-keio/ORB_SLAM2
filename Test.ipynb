{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc1preprocessed/100s-267s_fr3000-8000/outputGrid_base.mp4', '/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc1preprocessed/100s-267s_fr3000-8000/outputGrid_toolsAndOrgansRemoved_SAM_skelet0.mp4', '/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc1preprocessed/100s-267s_fr3000-8000/outputGrid_toolsRemoved.mp4']\n",
      "['/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc1preprocessed/100s-267s_fr3000-8000/outputAll_base.mp4', '/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc1preprocessed/100s-267s_fr3000-8000/outputAll_toolsAndOrgansRemoved_SAM_skelet0.mp4', '/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc1preprocessed/100s-267s_fr3000-8000/outputAll_toolsRemoved.mp4']\n",
      "['/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc1preprocessed/100s-267s_fr3000-8000/outputGrid_toolsAndOrgansRemoved_SAM_skelet0.mp4', '/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc1preprocessed/100s-267s_fr3000-8000/outputGrid_toolsAndOrgansRemoved_SAM_skelet1.mp4', '/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc1preprocessed/100s-267s_fr3000-8000/outputGrid_toolsAndOrgansRemoved_SAM_skelet2.mp4']\n",
      "['/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc1preprocessed/100s-267s_fr3000-8000/outputAll_toolsAndOrgansRemoved_SAM_skelet0.mp4', '/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc1preprocessed/100s-267s_fr3000-8000/outputAll_toolsAndOrgansRemoved_SAM_skelet1.mp4', '/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc1preprocessed/100s-267s_fr3000-8000/outputAll_toolsAndOrgansRemoved_SAM_skelet2.mp4']\n",
      "480 1440 270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x55a15c8b5200] moov atom not found\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x55a15b923040] moov atom not found\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x55a156e83100] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480 1440 270\n"
     ]
    }
   ],
   "source": [
    "# FUSE VIDEO ORBSLAM2\n",
    "def fuse_video(gridVideoPaths, outlierVideoPaths , outputPathFolder, onlySlelet = False):\n",
    "    # Read videos\n",
    "    gridVideos = []\n",
    "    outlierVideos = []\n",
    "    names = []\n",
    "    for gridVideoPath in gridVideoPaths:\n",
    "        gridVideos.append(cv2.VideoCapture(gridVideoPath))\n",
    "        names.append(gridVideoPath.split('/outputGrid_')[-1].split('.')[0])\n",
    "\n",
    "    for outlierVideoPath in outlierVideoPaths:\n",
    "        outlierVideos.append(cv2.VideoCapture(outlierVideoPath))\n",
    "\n",
    "    # Get video properties\n",
    "    fps = gridVideos[0].get(cv2.CAP_PROP_FPS)\n",
    "    width = int(gridVideos[0].get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(gridVideos[0].get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    if onlySlelet:\n",
    "        outString = 'outputCompareSkelet.mp4'\n",
    "        outStringAdd = 'outputAddWeightedSkelet.mp4'\n",
    "    else:\n",
    "        outString = 'outputCompare.mp4'\n",
    "        outStringAdd = 'outputAddWeighted.mp4'\n",
    "    out = cv2.VideoWriter(outputPathFolder + outString, fourcc, fps, (width * len(gridVideos), 2*height))\n",
    "    outSum = cv2.VideoWriter(outputPathFolder + outStringAdd, fourcc, fps, (width, height))\n",
    "    print(width,width * len(gridVideos), height)\n",
    "    # Read frames\n",
    "    totalFramesMax = 0\n",
    "    for video in gridVideos:\n",
    "        totalFramesMax = max(totalFramesMax, int(video.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "\n",
    "    for frameIndex in range(totalFramesMax):\n",
    "        # Read frame\n",
    "        gridFrames = []\n",
    "        outlierFrames = []\n",
    "        for video in gridVideos:\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, frameIndex)\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "            gridFrames.append(frame)\n",
    "\n",
    "        for video in outlierVideos:\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, frameIndex)\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "            outlierFrames.append(frame)\n",
    "\n",
    "        # sum\n",
    "        gridFrameSum = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        for frame in gridFrames:\n",
    "            gridFrameSum += frame // len(gridFrames)\n",
    "\n",
    "        # Concatenate frames\n",
    "        gridFrame = np.concatenate(gridFrames, axis=1)\n",
    "        outlierFrame = np.concatenate(outlierFrames, axis=1)\n",
    "        frame = np.concatenate([gridFrame, outlierFrame], axis=0)\n",
    "\n",
    "        # Write names\n",
    "        for i, name in enumerate(names):\n",
    "            cv2.putText(frame, name, (width * i + 10, height), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Write frame\n",
    "        out.write(frame)\n",
    "        outSum.write(gridFrameSum)\n",
    "\n",
    "    # Release videos\n",
    "    for video in gridVideos:\n",
    "        video.release()\n",
    "    for video in outlierVideos:\n",
    "        video.release()\n",
    "\n",
    "    # Release output video\n",
    "    out.release()\n",
    "    outSum.release()\n",
    "\n",
    "folderPath = \"/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc1preprocessed/100s-267s_fr3000-8000/\" \n",
    "gridVideoPathsOnlySkelet = glob.glob(folderPath + \"outputGrid*skelet*.mp4\")\n",
    "outlierVideoPathsOnlySkelet = glob.glob(folderPath + \"outputAll*skelet*.mp4\")\n",
    "gridVideoPaths = glob.glob(folderPath + \"outputGrid*.mp4\")\n",
    "filteredGrid = [x for x in gridVideoPaths if \"skelet1\" not in x and \"skelet2\" not in x]\n",
    "outlierVideoPaths = glob.glob(folderPath + \"outputAll*.mp4\")\n",
    "filteredOutlier = [x for x in outlierVideoPaths if \"skelet1\" not in x and \"skelet2\" not in x]\n",
    "filteredOutlier.sort()\n",
    "filteredGrid.sort()\n",
    "gridVideoPathsOnlySkelet.sort()\n",
    "outlierVideoPathsOnlySkelet.sort()\n",
    "print(filteredGrid)\n",
    "print(filteredOutlier)\n",
    "print(gridVideoPathsOnlySkelet)\n",
    "print(outlierVideoPathsOnlySkelet)\n",
    "fuse_video(filteredGrid, filteredOutlier ,folderPath)\n",
    "fuse_video(gridVideoPathsOnlySkelet, outlierVideoPathsOnlySkelet ,folderPath, onlySlelet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc2/333s-400s_fr10000-12000/outputGrid_base.mp4', '/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc2/333s-400s_fr10000-12000/outputGrid_toolsAndOrgansRemoved_SAM.mp4', '/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc2/333s-400s_fr10000-12000/outputGrid_toolsRemoved.mp4']\n",
      "['/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc2/333s-400s_fr10000-12000/outputAll_base.mp4', '/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc2/333s-400s_fr10000-12000/outputAll_toolsAndOrgansRemoved_SAM.mp4', '/home/jbjoannic/Documents/Recherche/presentations/meeting 17 janvier/videos grids/lapc2/333s-400s_fr10000-12000/outputAll_toolsRemoved.mp4']\n",
      "480 1440 270\n"
     ]
    }
   ],
   "source": [
    "##FUSE VIDEO TESTLIBTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jbjoannic/Documents/Recherche/orbSlam/test-libtorch/results/skelet/videoSAM0.avi', '/home/jbjoannic/Documents/Recherche/orbSlam/test-libtorch/results/skelet/videoSAM1.avi', '/home/jbjoannic/Documents/Recherche/orbSlam/test-libtorch/results/skelet/videoSAM2.avi']\n",
      "480 1440 270\n"
     ]
    }
   ],
   "source": [
    "##FUSE VIDEO TESTLIBTORCH\n",
    "\n",
    "# FUSE VIDEO ORBSLAM2\n",
    "def fuse_video(SAMVideoPaths, outputPathFolder):\n",
    "    # Read videos\n",
    "    SAMVideos = []\n",
    "    names = []\n",
    "    for SAMVideoPath in SAMVideoPaths:\n",
    "        SAMVideos.append(cv2.VideoCapture(SAMVideoPath))\n",
    "        names.append(SAMVideoPath.split('/video')[-1].split('.')[0])\n",
    "\n",
    "\n",
    "    # Get video properties\n",
    "    fps = SAMVideos[0].get(cv2.CAP_PROP_FPS)\n",
    "    width = int(SAMVideos[0].get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(SAMVideos[0].get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(outputPathFolder + 'SAMCompare.mp4', fourcc, fps, (width * len(SAMVideos), height))\n",
    "    print(width,width * len(SAMVideos), height)\n",
    "    # Read frames\n",
    "    totalFramesMax = 0\n",
    "    for video in SAMVideos:\n",
    "        totalFramesMax = max(totalFramesMax, int(video.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "\n",
    "    for frameIndex in range(totalFramesMax):\n",
    "        # Read frame\n",
    "        SAMFrames = []\n",
    "        outlierFrames = []\n",
    "        for video in SAMVideos:\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, frameIndex)\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "            SAMFrames.append(frame)\n",
    "\n",
    "\n",
    "        # Concatenate frames\n",
    "        SAMFrame = np.concatenate(SAMFrames, axis=1)\n",
    "\n",
    "        # Write names\n",
    "        for i, name in enumerate(names):\n",
    "            cv2.putText(SAMFrame, name, (width * i + 10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Write frame\n",
    "        out.write(SAMFrame)\n",
    "\n",
    "    # Release videos\n",
    "    for video in SAMVideos:\n",
    "        video.release()\n",
    "\n",
    "    # Release output video\n",
    "    out.release()\n",
    "\n",
    "folderPath = \"/home/jbjoannic/Documents/Recherche/orbSlam/test-libtorch/results/skelet/\" \n",
    "SAMVideoPaths = glob.glob(folderPath + \"videoSAM*.avi\")\n",
    "SAMVideoPaths.sort()\n",
    "print(SAMVideoPaths)\n",
    "fuse_video(SAMVideoPaths ,folderPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
